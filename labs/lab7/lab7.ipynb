{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wzum_lab7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 7 - few-shot learning and hypernetworks\n",
        "\n",
        "Plan for today:\n",
        "* learn about the concept of few-shot learning\n",
        "* familiarize ourselves with hypernetworks\n",
        "* connect those two concepts by implementing a technique from [this paper](https://arxiv.org/pdf/1706.03466.pdf)."
      ],
      "metadata": {
        "id": "CFPJebZLiTr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install learn2learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HkUkXXVTkSUy",
        "outputId": "0c9849f8-bfa1-4804-e677-47b9264953dc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting learn2learn\n",
            "  Using cached learn2learn-0.1.7-cp37-cp37m-linux_x86_64.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from learn2learn) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from learn2learn) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from learn2learn) (1.4.1)\n",
            "Collecting gsutil\n",
            "  Using cached gsutil-5.10-py3-none-any.whl\n",
            "Requirement already satisfied: torchvision>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from learn2learn) (0.12.0+cu113)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from learn2learn) (1.11.0+cu113)\n",
            "Requirement already satisfied: gym>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from learn2learn) (0.17.3)\n",
            "Collecting qpth>=0.0.15\n",
            "  Using cached qpth-0.0.15-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from learn2learn) (4.64.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.14.0->learn2learn) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.14.0->learn2learn) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.14.0->learn2learn) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.1.0->learn2learn) (4.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.3.0->learn2learn) (7.1.2)\n",
            "Collecting google-auth[aiohttp]>=2.5.0\n",
            "  Using cached google_auth-2.6.6-py2.py3-none-any.whl (156 kB)\n",
            "Collecting pyOpenSSL>=0.13\n",
            "  Using cached pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
            "Collecting argcomplete>=1.9.4\n",
            "  Using cached argcomplete-2.0.0-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from gsutil->learn2learn) (1.15.0)\n",
            "Collecting google-apitools>=0.5.32\n",
            "  Using cached google_apitools-0.5.32-py3-none-any.whl (135 kB)\n",
            "Collecting retry-decorator>=1.0.0\n",
            "  Using cached retry_decorator-1.1.1-py2.py3-none-any.whl\n",
            "Collecting gcs-oauth2-boto-plugin>=3.0\n",
            "  Using cached gcs_oauth2_boto_plugin-3.0-py3-none-any.whl\n",
            "Collecting fasteners>=0.14.1\n",
            "  Using cached fasteners-0.17.3-py3-none-any.whl (18 kB)\n",
            "Collecting httplib2>=0.20.4\n",
            "  Using cached httplib2-0.20.4-py3-none-any.whl (96 kB)\n",
            "Collecting monotonic>=1.4\n",
            "  Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: crcmod>=1.7 in /usr/local/lib/python3.7/dist-packages (from gsutil->learn2learn) (1.7)\n",
            "Collecting google-reauth>=0.1.0\n",
            "  Using cached google_reauth-0.1.1-py2.py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-metadata<5,>=0.23 in /usr/local/lib/python3.7/dist-packages (from argcomplete>=1.9.4->gsutil->learn2learn) (4.11.3)\n",
            "Requirement already satisfied: oauth2client>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from gcs-oauth2-boto-plugin>=3.0->gsutil->learn2learn) (4.1.3)\n",
            "Collecting rsa==4.7.2\n",
            "  Using cached rsa-4.7.2-py3-none-any.whl (34 kB)\n",
            "Collecting boto>=2.29.1\n",
            "  Using cached boto-2.49.0-py2.py3-none-any.whl (1.4 MB)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa==4.7.2->gcs-oauth2-boto-plugin>=3.0->gsutil->learn2learn) (0.4.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth[aiohttp]>=2.5.0->gsutil->learn2learn) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth[aiohttp]>=2.5.0->gsutil->learn2learn) (0.2.8)\n",
            "Collecting aiohttp<4.0.0dev,>=3.6.2\n",
            "  Using cached aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]>=2.5.0->gsutil->learn2learn) (1.7.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]>=2.5.0->gsutil->learn2learn) (21.4.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]>=2.5.0->gsutil->learn2learn) (6.0.2)\n",
            "Collecting asynctest==0.13.0\n",
            "  Using cached asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]>=2.5.0->gsutil->learn2learn) (2.0.12)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]>=2.5.0->gsutil->learn2learn) (1.3.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Using cached aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting pyu2f\n",
            "  Using cached pyu2f-0.1.5-py3-none-any.whl\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.7/dist-packages (from httplib2>=0.20.4->gsutil->learn2learn) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5,>=0.23->argcomplete>=1.9.4->gsutil->learn2learn) (3.8.0)\n",
            "Collecting cryptography>=35.0\n",
            "  Using cached cryptography-37.0.2-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=35.0->pyOpenSSL>=0.13->gsutil->learn2learn) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=35.0->pyOpenSSL>=0.13->gsutil->learn2learn) (2.21)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->learn2learn) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->learn2learn) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->learn2learn) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->learn2learn) (3.0.4)\n",
            "Installing collected packages: rsa, pyu2f, httplib2, cryptography, asynctest, async-timeout, aiosignal, retry-decorator, pyOpenSSL, google-reauth, google-auth, fasteners, boto, aiohttp, monotonic, google-apitools, gcs-oauth2-boto-plugin, argcomplete, qpth, gsutil, learn2learn\n",
            "  Attempting uninstall: rsa\n",
            "    Found existing installation: rsa 4.8\n",
            "    Uninstalling rsa-4.8:\n",
            "      Successfully uninstalled rsa-4.8\n",
            "  Attempting uninstall: httplib2\n",
            "    Found existing installation: httplib2 0.17.4\n",
            "    Uninstalling httplib2-0.17.4:\n",
            "      Successfully uninstalled httplib2-0.17.4\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 1.35.0\n",
            "    Uninstalling google-auth-1.35.0:\n",
            "      Successfully uninstalled google-auth-1.35.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-api-core 1.31.5 requires google-auth<2.0dev,>=1.25.0, but you have google-auth 2.6.6 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 argcomplete-2.0.0 async-timeout-4.0.2 asynctest-0.13.0 boto-2.49.0 cryptography-37.0.2 fasteners-0.17.3 gcs-oauth2-boto-plugin-3.0 google-apitools-0.5.32 google-auth-2.6.6 google-reauth-0.1.1 gsutil-5.10 httplib2-0.20.4 learn2learn-0.1.7 monotonic-1.6 pyOpenSSL-22.0.0 pyu2f-0.1.5 qpth-0.0.15 retry-decorator-1.1.1 rsa-4.7.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.datasets import Omniglot, EMNIST\n",
        "from torchvision import transforms as T\n",
        "import learn2learn as l2l\n",
        "import matplotlib.pyplot as plt\n",
        "from learn2learn.data import MetaDataset, TaskDataset\n",
        "from learn2learn.vision.models import OmniglotCNN\n",
        "from torch import nn\n",
        "from typing import Tuple\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "A5seZO8iiXtY"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Few-shot learning\n",
        "\n",
        "In general, neural networks require huge amounts of data to train well. Few-shot learning techniques aim to construct models, which are capable of quickly adapting to numerous **tasks** based on limited amounts of data.\n",
        "\n",
        "One of the most popular usecases for FSL is image classification. We define $K$-shot, $N$-way classification as the task of classifying between $N$ classes based on $K$ examples for each of the classes, called the **support set**. The model is then tasked with classifying the **query set** of previously unseen images, which belong to the same set of $N$ classes.\n",
        "\n",
        "During training, we construct **tasks** consisting of support and query examples from a set of training classes and taks the model with adapting to those tasks.\n",
        "\n",
        "We evaluate the model on tasks sampled from a set of classes **separate from the training set** - after all, we want to measure how well the model adapts to previously unseen tasks!\n",
        "\n",
        "\n",
        "\n",
        "One of the most popular datasets for FSL is Omniglot. \n",
        "\n",
        "### Task for you - import the omniglot dataset from the [learn2learn](http://learn2learn.net/) package and visualize an example task:\n",
        "* sample a single task from the tasksets\n",
        "* draw a grid with images and their classes"
      ],
      "metadata": {
        "id": "WT2nPXZNdouR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shots = 5\n",
        "queries = 15\n",
        "ways = 5\n",
        "\n",
        "\n",
        "tasksets = l2l.vision.benchmarks.get_tasksets('omniglot',\n",
        "                                                  train_ways=ways,\n",
        "                                                  train_samples=shots + queries,\n",
        "                                                  test_ways=ways,\n",
        "                                                  test_samples=shots + queries,\n",
        "                                                  num_tasks=20000,\n",
        "                                                  root='~/data',\n",
        "                                              \n",
        "    )\n",
        "\n",
        "for X, y in tasksets.train:\n",
        "  break\n",
        "\n",
        "# your code here - visualize the examples from X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ez1HC-Cpi3ZV",
        "outputId": "ef13bb9d-b314-4105-cd94-3bd262858a34"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:333: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hypernetworks\n",
        "\n",
        "Hypernetworks are models which, based on some condition, predict weights of other neural networks, which perform the downstream tasks. The concept has been utilized in many fields, such as generative models, point clouds, condtional flows, as well as few-shot learning.\n",
        "\n",
        "\n",
        "## Bringing it all together - Parameter Prediction from Activations\n",
        "\n",
        "Today, we will utilize the hypernets in the task of FSL. We will base our solution on the [Few-Shot Image Recognition by Predicting Parameters from Activations](https://arxiv.org/pdf/1706.03466.pdf).\n",
        "\n",
        "The PPA model consists of:\n",
        "* a convolutional backbone\n",
        "* a parameter prediction hypernet\n",
        "\n",
        "First, we process the support and query samples through the backbone and obtain embeddings $E$ Next, we want to predict the weights of the classifier which will transform $E$ into classes $C$. The classifier is therefore a linear layer with dimentionality $(E, C)$.\n",
        "\n",
        "We can predict the weights of the classifier in several ways:\n",
        "* concatenate all of the support embeddings and predict all of the classifier parameters\n",
        "* predict the *portion* of parameters of shape $(E, 1)$ dedicated to predicting class $C$ based only on the support embeddings from that class. Then, concatenate all portions into weights of shape $(E, C)$.\n",
        "\n"
      ],
      "metadata": {
        "id": "jO4fqMv2i4JM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task for you - implement the few-shot hypernetwork\n",
        "* implement two variants of classifier generation:\n",
        "  * generating **all** weights based on **all** support class embeddings\n",
        "  * generating **weight fragments** responsible for predicting class $C$ based solely on support embbedings of class $C$\n"
      ],
      "metadata": {
        "id": "FbStdIILYr_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Hypernet(nn.Module):\n",
        "  def __init__(self, n_shot: int, n_way: int, hidden_size: int = 64, weights_per_class: bool = True):\n",
        "    super().__init()\n",
        "    self.cnn = OmniglotCNN(hidden_size=hidden_size).features \n",
        "    # a convolutional net which transforms an image of shape (1, 28, 28) to vectors of shape `hidden_size`\n",
        "\n",
        "    self.weight_predictor = ... \n",
        "\n",
        "  def forward(\n",
        "      self, \n",
        "      support_examples: torch.Tensor, \n",
        "      support_labels: torch.Tensor,\n",
        "      query_examples: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    support_examples: [n_shot * n_way, 1, 28, 28]\n",
        "    support_labels: [n_shot * n_way]\n",
        "    query_samples: [n_query]\n",
        "\n",
        "    Returns a tuple of logits:\n",
        "      (y_pred_support, y_pred_query)\n",
        "      of shapes:\n",
        "      (\n",
        "        [n_shot * n_way, n_way],\n",
        "        [n_query, n_way]        \n",
        "      ) \n",
        "    \"\"\"\n",
        "\n",
        "    # 1: process the supports and queries through the cnn\n",
        "    # 2: generate the weights of the classifier based on the support embeddings\n",
        "    # 3: classify the support and query embeddings with the generated weights\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UTGOmMmVQ_ds"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shots = 5\n",
        "queries = 15\n",
        "ways = 5\n",
        "\n",
        "\n",
        "tasksets = l2l.vision.benchmarks.get_tasksets('omniglot',\n",
        "                                                  train_ways=ways,\n",
        "                                                  train_samples=shots + queries,\n",
        "                                                  test_ways=ways,\n",
        "                                                  test_samples=shots + queries,\n",
        "                                                  num_tasks=20000,\n",
        "                                                  root='~/data',\n",
        "                                              \n",
        "    )"
      ],
      "metadata": {
        "id": "xIeWSusbW4f_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task for you - finish implementing the training loop:\n",
        "* add the necessary training loss and optimizer parts\n",
        "* track the meta-training and meta-validation losses and accuracies throughout the training epochs and plot them after the training\n",
        "* train the two variants of hypernetwork on Omniglot \n",
        "* train the hypernets in two settings:\n",
        "  * 1-shot, 5-way\n",
        "  * 5-shot, 5-way"
      ],
      "metadata": {
        "id": "Pi1Jsaz8YFAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_hypernet(\n",
        "    hypernet: Hypernet,\n",
        "    tasksets,\n",
        "    optimizer,\n",
        "    num_epochs: int = 20,\n",
        "    n_shot: int = shots,\n",
        "    n_query: int = queries,\n",
        "    n_ways: int = ways,\n",
        "    img_shape = (1, 28, 28)\n",
        "):\n",
        "\n",
        "  for e in range(num_epochs):\n",
        "    # meta-training:\n",
        "\n",
        "    for X, y in tasksets.train:\n",
        "      # reshape X and y to have each class in a separate row\n",
        "      X = X.reshape(n_ways, n_shot+n_query, *img_shape)\n",
        "      y = y.reshape(n_ways, n_shot+n_query,)\n",
        "\n",
        "      # separate support from query\n",
        "      X_support, X_query = X[:, :n_shot], X[:, n_shot:]\n",
        "      y_support, y_query = y[:, :n_shot], y[: ,n_shot:]\n",
        "\n",
        "      # re-flatten the tensors\n",
        "      X_support = X_support.reshape(n_ways * n_shot, *img_shape)\n",
        "      X_query = X_query.reshape(n_ways * n_query, *img_shape)\n",
        "      y_support = y_support.reshape(n_ways * n_shot)\n",
        "      y_query = y_query.reshape(n_ways * n_query)\n",
        "\n",
        "\n",
        "      # predictions\n",
        "      y_support_pred, y_query_pred = hypernet(X_support, y_support, X_query)\n",
        "\n",
        "      # YOUR CODE HERE\n",
        "      # \n",
        "      #####\n",
        "      \n",
        "\n",
        "\n",
        "    # meta-validation\n",
        "    for X, y in tasksets.train:\n",
        "      # reshape X and y to have each class in a separate row\n",
        "      X = X.reshape(n_ways, n_shot+n_query, *img_shape)\n",
        "      y = y.reshape(n_ways, n_shot+n_query,)\n",
        "\n",
        "      # separate support from query\n",
        "      X_support, X_query = X[:, :n_shot], X[:, n_shot:]\n",
        "      y_support, y_query = y[:, :n_shot], y[: ,n_shot:]\n",
        "\n",
        "      # re-flatten the tensors\n",
        "      X_support = X_support.reshape(n_ways * n_shot, *img_shape)\n",
        "      X_query = X_query.reshape(n_ways * n_query, *img_shape)\n",
        "      y_support = y_support.reshape(n_ways * n_shot)\n",
        "      y_query = y_query.reshape(n_ways * n_query)\n",
        "\n",
        "      # YOUR CODE HERE\n",
        "      # \n",
        "      ####\n",
        "\n",
        "  # plot the training / validation losses and accuracies\n",
        "  "
      ],
      "metadata": {
        "id": "2S1o2wd0Q3_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize and train the hypernetwork"
      ],
      "metadata": {
        "id": "kQL1gfVol0x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question for you** - which variant of the few-shot hypernetwork worked better? Why?"
      ],
      "metadata": {
        "id": "P0OM96Qfln-U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final validation\n",
        "\n",
        "Let's validate our models on one more dataset - EMNIST - which contains digits and latin alphabet characters\n",
        "\n",
        "### Task for you\n",
        "* based on [documentation](http://learn2learn.net/tutorials/task_transform_tutorial/transform_tutorial/), prepare the EMNIST meta-dataset. Then, calculate the accuracy of the hypernetworks you've trained on the tasks from that dataset."
      ],
      "metadata": {
        "id": "KdW7nq_9loDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Rlo3tSADlgb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GV2m_kSGnlPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From 27.05 - project presentations!\n",
        "* Guidelines are [here](https://docs.google.com/document/d/1Xr49OjhKMTZu1Cxmz3b1exezXf9OWDgnlo3IzuYEhtw/edit)"
      ],
      "metadata": {
        "id": "h-ozSaJum1Aj"
      }
    }
  ]
}